# Starter Kit
## DriveML Huawei Autonomous Vehicles Challenge

Welcome! This starter kit is here to get you going to submit solutions to the DriveML challenge. The following instructions include simulator setup, solution examples, scenario details, submission process, and how to see your leaderboard score.

## OpenAI Gym Environment Setup

To setup the simulator (called HiWay) run the following commands,

```bash
# unzip the starter_kit and place somewhere convenient on your machine. (e.x. ~/src/starter_kit)

cd ~/src/starter_kit
./install_deps.sh
# ...and, follow any on-screen instructions

# test that the sumo installation worked
sumo-gui

# (optional) setup virtual environment
python3 -m venv .venv
source .venv/bin/activate
pip install --upgrade pip

# install the hiway python package
pip install hiway-0.1-py3-none-any.whl
pip install hiway-0.1-py3-none-any.whl[train]

# test that the sim works
python3
>> from hiway.sumo_scenario import SumoScenario
>> import gym

# download the public datasets from Codalab to ../dataset_public
>> env = gym.make('gym_hiway:hiway-competition-v0', config={'sumo_scenario': SumoScenario('../dataset_public/3lane')})

# you should see a black window open

>> env.reset()

# you should now see a loop with a vehicle on the road

>> [env.step([1., 0., 45.]) for i in range(100)]

# you should see the vehicle make a left turn
```

Next we will start running the simulator locally with different agents across different scenarios.

## Solution Examples

We provide two solution examples to get your started with experimenting. “Random” is a random agent policy to demonstrate what the most minimal solution would be. “RLlib” demonstrates a complete solution using [RLlib](https://ray.readthedocs.io/en/latest/rllib.html) to train a policy using [PPO](https://openai.com/blog/openai-baselines-ppo/).

### Random

To run the random agent,

```bash
python3 ~/src/starter_kit/random_example/run.py
```

The output should look something like this,
```bash
Retrying in 1 seconds
...
simulation ended
Accumulated reward: 11.999999999999993
```

If you open up the `run.py` script you can edit the scenario (see ”The Scenario > Dataset” section below) for all the available ones, update the number of social vehicles, max step length, and the seed. These will all be useful later when you train your own agent. You can also toggle headless mode, and `visdom` visualization (see “Appendix > Visualization” section below).

### RLlib

Bundled in the starter kit is a more sophisticated example demoing how to use the (RLib)[https://ray.readthedocs.io/en/latest/rllib.html] framework to train an agent.

RLlib supports both TensorFlow and PyTorch. It provides features for easily setting up and running experiments across many distributed nodes or across all CPU’s on a single node.

The RLlib example is composed of up into 3 files, the `policy.py`, `trainer.py` and `run.py` and a `model` directory.

* `policy.py` defines the,
    * observation/reward/action shaping functions
    * the model architecture
    * and some code to build a Policy class that we can use for evaluating the trained model
* `trainer.py` demonstrates how to setup an RLlib experiment
* `run.py` will use the Policy class defined in policy.py to evaluate the trained model
* `model/` contains a pre-trained network that was generated by trainer.py

To train a new model, first backup the current model in rllib_example/model and run:
```bash
python3 ~/src/starter_kit/rllib_example/trainer.py ~/src/dataset_public/3lane
# replace 3lane with the scenario to train against
```
This will train a new model and output it to the rllib_example/model/ directory.

The trainer script has a few options to assist with training
```bash
--headless  # runs the simulator in headless mode
--resume_training # resumes the previously started experiment
--num_social_vehicles <N> # the number of social vehicles to add to the experiment
```
NOTE: `--resume_training` will continue to use the same configuration as was set in the original experiment, ie. headless mode / number of social vehicles will remain unchanged.
To make changes to a started experimented, you can edit the latest experiment file in `~/ray_results/rllib_example` or use the checkpointing feature in RLLib to start a new experiment using a checkpointed model.

To evaluate a model:
```bash
Python3 ~/src/starter_kit/rllib_example/run.py
```
Building Your Own Policies
Building on top of the previous examples, or starting from scratch you can create your own policies by implementing the `Policy` interface. The `run.py` script (like in the previous examples), and CodaLab evaluation import this policy class. The interface is simple,

```python
class Policy():
    def setup(self):
        # called once after import and can be used to load your model

    def teardown(self):
        # clean-up any resources

    def act(self, observation):
        # takes an observation, and returns an action
```

See the following sections on what the space of the observation input looks like, the action space your response must be in, and the reward shaping functionality available to you.
[OpenAI Gym](https://gym.openai.com/) Environment Configuration Options
The simulation can be configured through the config dictionary passed to the gym.make call

```
env = gym.make('gym_hiway:hiway-competition-v0', config={
      "max_step_length": Int or None (default: None), maxsteps per episode, set to None for no max
      "visdom": bool (False): whether or not to enable visdom visualization (see Appendix).
      "headless": bool (False): disable gui.
      "seed": int (0): RNG Seed, seeds are set at the start of simulation, and never automatically re-seeded.
      "sumo_scenario": SumoScenario() the scenario to run.

      # see Shaping for an explanation on the following configuration functions
      "reward_function": function to shape reward returned by the environment
      "observation_function": function to shape observation returned by the environment
      "action_function": function to translate policy action space to environment action space
      "observation_space": The gym.Space of the observation_function output.
      "action_space": The gym.Space of the action_function input.
    })
```

## Observation Space

The raw observation space is a Python `namedtuple` with the following fields,

* `ego_vehicle_state` - a `VehicleState` `namedtuple` with the following fields,
    * `heading` - vehicle heading in degrees
    * `speed` - agent speed in km/h
    * `throttle` - a normalized engine force value
    * `brake` - a normalized brake force value
    * `steering` - steering value of vehicle
    * `position` - 3D numpy array (x, y, z) of vehicle
    * `bounding_box` - `BoundingBox` `namedtuple` for the `width`, `length`, `height`
* `neighborhood_vehicle_states` - a list of `SocialVehicleState` `namedtuple`s, each with the following fields,
    * `heading`, `speed`, `position`, `bounding_box` - same as with the `ego_vehicle_state`
    * `lane_id` - a global unique identifier of the lane under this waypoint
    * `lane_index` - index of the lane under this waypoint, right most lane has index 0 and the index increments to the left
* `top_down_rgb` - A 256x256 RGB image following the ego vehicle
* `occupancy_grid_map` - A 64x64 [OGM](https://en.wikipedia.org/wiki/Occupancy_grid_mapping) map following around the ego vehicle
* `waypoint_paths` - A list of waypoints in front of the ego vehicle showing the potential routes ahead. Each item is a `Waypoint` instance with the following fields,
    * `id` - an integer identifier for this waypoint
    * `pos` - a numpy array (x, y) center point along the lane
    * `heading` - heading angle of lane at this point (degrees)
    * `lane_width` - width of lane at this point (meters)
    * `speed_limit` - lane speed in km/h
    * `lane_id` - a global unique identifier of lane under waypoint
    * `right_of_way` - `True` if this waypoint has right of way, `False` otherwise
    * `lane_index` - index of the lane under this waypoint, right most lane has index 0 and the index increments to the left

## Action Space

The action space is a tuple of `throttle` [0, 1], `brake` [0, 1], and `steer` [-45, 45].

## Shaping

Frameworks like RLlib want to control the running of the OpenAI gym environment. The framework does not leave space between the environment and your model to make your necessary mappings between the Gym Env observations and actions and the model’s input/outputs.

A concrete example of this in action is the `rllib_example/policy.py` file in this starter_kit.

The shaping functions are passed into the HiWay Gym Environment as configuration options

```python
# This action space should match the input to the action(..) function below.
ACTION_SPACE = gym.spaces.Box(low=-1.0, high=1.0, shape=(1,))

# This observation space should match the output of observation(..) function below
OBSERVATION_SPACE = gym.spaces.Dict({
    'angle_error': gym.spaces.Box(low=-180, high=180, shape=(1,)),
})


def observation(env_obs):
    # Transform the environment's observation into OBSERVATION_SPACE
    ...
    return {'angle_error': np.array([angle_error])}

def reward(env_obs, env_reward):
    # perform any reward shaping you would like in here
    ...
    return reward

def action(model_action):
    # convert the model’s output (ACTION_SPACE) into the environments action space
    return np.array([0.1, 0., model_action[0] * 45])

# Now pass the these shaping functions to the environment.
# On each step, the environment will run the base observations, rewards, actions through your functions to returning the transformed results
gym.make('gym_hiway:hiway-competition-v0', config={
    ...
    'observation_space': OBSERVATION_SPACE,
    'action_space': ACTION_SPACE,
    'reward_function': reward,
    'observation_function': observation,
    'action_function': action,
})
```

## Dataset

Alongside the starter kit a public dataset is available on CodaLab. It’s up to you how to split this dataset up for training, testing, etc. The following scenarios are provided,

* `1lane` - the simplest scenario, a one-lane loop
* `1lane_sharp` - a one-lane loop, with sharp curvature
* `2lane_bwd` - a two-lane loop going backwards
* `2lane_sharp` - a two-lane loop with sharp curvature
* `3lane` - a simple three-lane loop
* `3lane_bwd_b` - a simple three-lane loop going backwards with a different shape than `3lane`
* `3lane_sharp` - a three-lane loop with sharp curvature
* `3lane_sharp_bwd_b` - a three-lane loop with sharp curvature, going backwards, and a different shape than `3lane-sharp`

In addition to these scenarios you can set the number of social vehicles, max step length, and the random number generator seed in the `run.py` script. Consider also how you can introduce episodes (looping after environment returns done) to continue training for longer periods of time. The environment returns `done` when the agent is off road, gets into an accident, or hits the max step length. All these params together will allow you to build larger more varied datasets.

## Evaluation

When you submit your solution we’ll put it through a similar evaluation to your local `run.py script`. However we’ll be evaluating it across a different set of scenarios: `1lane`, `1lane_10v`, `2lane_sharp_bwd_10v`, `3lane_bwd_10v`, `3lane_sharp_b_10v`, `3lane_sharp_b_25v`, `3lane_sharp_b_50v`. The `_XXv` suffix is the number of social vehicles running in the environment. We also run with a different seed, max step count, and episode count.

## Submission Process

When you’re happy with your solution and ready to submit you’ll want to zip your policy (and any associated files) and upload to CodaLab under “Participate > Submit/View Files”. Important: zip together just the files, not a directory with the files in it. Be careful to make sure your solutions run locally, and perform well before submitting as the upload limit is fixed.

### Leaderboard

If your solution succeeds it will automatically get posted to the CodaLab leaderboard under the results tab. You’ll see your score across all the evaluation scenarios and a final rank score which is the sum of them all. This is the score in which the winners will be chosen from.

## Appendix

### Visualization

We add built-in support for [Visdom](https://github.com/facebookresearch/visdom) so you can see the image-based observation outputs in real-time. Start the visdom server before running your scenario and open the server URL in your browser `http://localhost:8097`.

```bash
# (optional) source your virtual environment
cd ~/src/starter_kit
source .venv/bin/activate

# install visdom
pip install visdom

# start the server
visdom
```

In your `run.py` script enable `visdom` with,

```python
env = gym.make('gym_hiway:hiway-competition-v0',
               config={
                   ...
                   'visdom': False,
                   ...
               })
```
